{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbaf2329-5a49-459f-a5a9-ae53cb655758",
   "metadata": {},
   "source": [
    "Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556caf20-7463-4d46-b6c4-7e9f938bf112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\obhim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\obhim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\obhim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\obhim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482e0aa-4a21-48ba-bc06-cd93b5050ba7",
   "metadata": {},
   "source": [
    "Import the dataset and clean the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752a25a6-edc1-49ba-aaf6-7133c0c9de32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20972, 9)\n",
      "(8989, 3)\n"
     ]
    }
   ],
   "source": [
    "train= pd.read_csv(\"train.csv\")\n",
    "test= pd.read_csv(\"test.csv\")\n",
    "train.head(2)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "col = ['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']\n",
    "test = test.drop(['ID'],axis=1)\n",
    "\n",
    "X = train.loc[:,['TITLE','ABSTRACT']]\n",
    "y = train.loc[:,col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053391e8-0bc3-4cb4-9581-ef0f98081da6",
   "metadata": {},
   "source": [
    "Prepare the dataset for fitting in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c957698-040f-4770-8762-3f24f3f0f413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18874, 2) (2098, 2)\n",
      "(18874, 6) (2098, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "y_test.reset_index(drop=True,inplace=True)\n",
    "X_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "y1 = np.array(y_train)\n",
    "y2 = np.array(y_test)\n",
    "#Removing Punctuations\n",
    "\n",
    "X_train.replace('[^a-zA-Z]',' ', regex=True, inplace=True)\n",
    "X_test.replace('[^a-zA-Z]',' ', regex=True, inplace=True)\n",
    "\n",
    "test.replace('[^a-zA-Z]',' ', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fbf08cd-1cfb-42b0-8782-45cc35dbb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to lower case characters\n",
    "\n",
    "for index in X_train.columns:\n",
    "  X_train[index] = X_train[index].str.lower()\n",
    "\n",
    "for index in X_test.columns:\n",
    "  X_test[index] = X_test[index].str.lower()\n",
    "\n",
    "for index in test.columns:\n",
    "  test[index] = test[index].str.lower()\n",
    "\n",
    "#Removing one letter words\n",
    "\n",
    "X_train['ABSTRACT'] = X_train['ABSTRACT'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "X_test['ABSTRACT'] = X_test['ABSTRACT'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "\n",
    "test['ABSTRACT'] = test['ABSTRACT'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "\n",
    "#Removing multiple blank spaces\n",
    "\n",
    "X_train = X_train.replace(r's+', ' ', regex=True)\n",
    "X_test = X_test.replace(r's+', ' ', regex=True)\n",
    "\n",
    "test = test.replace(r's+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14363a37-e1d0-4a0e-aa29-b0367b19b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back to form a preprocessed text\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf0a8d5-bd32-46a9-8db4-e3073b1b7630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lines(data):\n",
    "    lines = []\n",
    "    for row in range(data.shape[0]):\n",
    "        lines.append(' '.join(str(x) for x in data.iloc[row, :]))\n",
    "    return lines\n",
    "stop_words = set(stopwords.words('english')) \n",
    "X_train['combined'] = X_train['TITLE']+' '+X_train['ABSTRACT']\n",
    "X_test['combined'] = X_test['TITLE']+' '+X_test['ABSTRACT']\n",
    "\n",
    "test['combined'] = test['TITLE']+' '+test['ABSTRACT']\n",
    "\n",
    "X_train = X_train.drop(['TITLE','ABSTRACT'],axis=1)\n",
    "X_test = X_test.drop(['TITLE','ABSTRACT'],axis=1)\n",
    "\n",
    "test = test.drop(['TITLE','ABSTRACT'],axis=1)\n",
    "\n",
    "X_train.head()\n",
    "X_lines = []\n",
    "for row in range(0,X.shape[0]):\n",
    "  X_lines.append(' '.join(str(x) for x in X.iloc[row,:]))\n",
    "\n",
    "train_lines = []\n",
    "for row in range(0,X_train.shape[0]):\n",
    "  train_lines.append(' '.join(str(x) for x in X_train.iloc[row,:]))\n",
    "\n",
    "test_lines = []\n",
    "for row in range(0,X_test.shape[0]):\n",
    "  test_lines.append(' '.join(str(x) for x in X_test.iloc[row,:]))\n",
    "\n",
    "predtest_lines = []\n",
    "for row in range(0,test.shape[0]):\n",
    "  predtest_lines.append(' '.join(str(x) for x in test.iloc[row,:]))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvector = CountVectorizer(ngram_range=(1,2))\n",
    "X_train_cv = countvector.fit_transform(train_lines)\n",
    "X_test_cv = countvector.transform(test_lines)\n",
    "\n",
    "test_cv = countvector.transform(predtest_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b673f757-e354-4f26-851d-c60d451c4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "tfidfvector = TfidfTransformer()\n",
    "X_train_tf = tfidfvector.fit_transform(X_train_cv)\n",
    "X_test_tf = tfidfvector.fit_transform(X_test_cv)\n",
    "\n",
    "test_tf = tfidfvector.fit_transform(test_cv)\n",
    "\n",
    "X_cv = countvector.transform(X_lines)\n",
    "\n",
    "X_tf = tfidfvector.fit_transform(X_cv) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77292d57-2db2-47fe-a90a-f4a0dbc8189a",
   "metadata": {},
   "source": [
    "Preparing Model and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec625370-974c-436d-bc2f-da42a1c608fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.85       853\n",
      "           1       0.89      0.88      0.89       623\n",
      "           2       0.83      0.83      0.83       580\n",
      "           3       0.73      0.85      0.78       516\n",
      "           4       0.49      0.40      0.44        58\n",
      "           5       0.81      0.65      0.72        26\n",
      "\n",
      "   micro avg       0.80      0.86      0.83      2656\n",
      "   macro avg       0.76      0.75      0.75      2656\n",
      "weighted avg       0.81      0.86      0.83      2656\n",
      " samples avg       0.84      0.88      0.84      2656\n",
      "\n",
      "0.6601525262154433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "model = LinearSVC(C=0.5, class_weight='balanced', random_state=42)\n",
    "models = MultiOutputClassifier(model)\n",
    "\n",
    "models.fit(X_train_tf, y1)\n",
    "preds = models.predict(X_test_tf)\n",
    "preds\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "#print(confusion_matrix(y2,preds))\n",
    "print(classification_report(y2,preds))\n",
    "print(accuracy_score(y2,preds))\n",
    "predssv = models.predict(test_tf)\n",
    "predssv\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "submit = pd.DataFrame({'ID': test.ID, 'Computer Science': predssv[:,0],'Physics':predssv[:,1],\n",
    "                       'Mathematics':predssv[:,2],'Statistics':predssv[:,3],'Quantitative Biology':predssv[:,4],\n",
    "                       'Quantitative Finance':predssv[:,5]})\n",
    "submit.head()\n",
    "submit.to_csv('obhi_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7fa6516-ca33-46ce-90fc-46093c14735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'estimator__C': 1, 'estimator__class_weight': 'balanced', 'estimator__loss': 'squared_hinge', 'estimator__max_iter': 1000, 'estimator__tol': 1e-05}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.85       853\n",
      "           1       0.90      0.88      0.89       623\n",
      "           2       0.85      0.82      0.84       580\n",
      "           3       0.73      0.83      0.77       516\n",
      "           4       0.56      0.34      0.43        58\n",
      "           5       0.79      0.58      0.67        26\n",
      "\n",
      "   micro avg       0.81      0.85      0.83      2656\n",
      "   macro avg       0.77      0.72      0.74      2656\n",
      "weighted avg       0.82      0.85      0.83      2656\n",
      " samples avg       0.84      0.87      0.84      2656\n",
      "\n",
      "Accuracy: 0.6625357483317446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# Define the extended parameter grid\n",
    "param_grid = {\n",
    "    'estimator__C': [0.001, 0.01, 0.1, 0.5, 1, 10, 100],  # Regularization parameter\n",
    "    'estimator__class_weight': [None, 'balanced'],\n",
    "    'estimator__loss': ['hinge', 'squared_hinge'],  # Loss function\n",
    "    'estimator__tol': [1e-5, 1e-4, 1e-3],  # Tolerance for stopping criteria\n",
    "    'estimator__max_iter': [1000, 2000, 3000],  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Create the base model\n",
    "base_model = LinearSVC(random_state=42)\n",
    "\n",
    "# Create the MultiOutputClassifier\n",
    "model = MultiOutputClassifier(base_model)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train_tf, y1)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds = best_model.predict(X_test_tf)\n",
    "\n",
    "# Print evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(classification_report(y2, preds))\n",
    "print(\"Accuracy:\", accuracy_score(y2, preds))\n",
    "\n",
    "# Make predictions on the final test set\n",
    "predssv = best_model.predict(test_tf)\n",
    "\n",
    "# Create the submission dataframe\n",
    "submit = pd.DataFrame({'ID': test.ID, 'Computer Science': predssv[:, 0],\n",
    "                       'Physics': predssv[:, 1], 'Mathematics': predssv[:, 2],\n",
    "                       'Statistics': predssv[:, 3], 'Quantitative Biology': predssv[:, 4],\n",
    "                       'Quantitative Finance': predssv[:, 5]})\n",
    "\n",
    "# Save the submission to a CSV file\n",
    "submit.to_csv('obhi_data_tuned_extended.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d0e99ed-48f7-4bda-baee-c990cc44dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save the trained MultiOutputClassifier model to a file\n",
    "with open('multi_output_classifier_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file) \n",
    "# Loading the MultiOutputClassifier model\n",
    "with open('multi_output_classifier_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Save the CountVectorizer\n",
    "with open('countvectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(countvector, file)\n",
    "# Loading the CountVectorizer\n",
    "with open('countvectorizer.pkl', 'rb') as file:\n",
    "    loaded_countvectorizer = pickle.load(file)\n",
    "\n",
    "# Save the TfidfTransformer\n",
    "with open('tfidftransformer.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidfvector, file)\n",
    "# Loading the TfidfTransformer\n",
    "with open('tfidftransformer.pkl', 'rb') as file:\n",
    "    loaded_tfidftransformer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54368946-6b7d-4a7d-bb45-2fecb32b9884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
